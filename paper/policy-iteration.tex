
\section{TOP with Policy Iteration}\label{sec: TOP-PI}
\noindent 
In this section we discuss how we instantiate our general model when
the learner is using a \emph{policy iteration} algorithm.  As we have
described in the previous section, the framework instantiation process
has two stages. We first identify the sufficient set of variables to
describe the computation state of the learning algorithm. We then
represent the algorithm's iteration that transforms this state in a
functional form.

Now, the standard policy iteration (PI) algorithm is an iterative algorithm
that operates over an explicitly given MDP\cite{puterman_book_94}.  It
consists of two principal stages:
\begin{description}
\item[Policy Evaluation] where the algorithm computes the reward of
  the agent if it applies its current policy in the given
  MDP. Specifically, given the policy of the previous iteration,
  $\pi_{t-1}$, the value function $V_t(s)$ for that policy is
  computed, where $V_t(s)$ represents the expected total discounted
  reward that can be achieved if the environment starts at state $s$
  and the agent follows $\pi_{t-1}$.

\item[Policy Improvement]  where the value function
of the current policy is used to guide the computation of the next
stage policy. Commonly, it is a policy, $\pi_{t}$, optimal with
respect to the current value function, $V_t(s)$.
\end{description}

%and
%it has two principal stages: policy evaluation and policy
%improvement. At the the {\em policy evaluation} stage PI computes the
%reward gained by applying currently considered policy. Specifically,
%given the policy of the previous iteration, $\pi_{t-1}$, the value
%function $V_t(s)$ for that policy is computed, where $V_t(s)$
%represents the expected total discounted reward that can be achieved
%if the environment starts at state $s$ and the agent follows
%$\pi_{t-1}$. At the {\em policy improvement} stage the value function
%of the current policy is used to guide the computation of the next
%stage policy. Commonly, it is a policy, $\pi_{t}$, optimal with
%respect to the currect value function, $V_t(s)$.

Since its original introduction, both stages of the algorithm have
been refined to allow for partial knowledge of the domain.  For
instance, the value function can be estimated, rather than computed,
in environments where a direct computation is too complex or
impossible due to poor modelling by the learner (see
e.g. ~\cite{vanRoy_98,koller_parr_2000,lagoudakis_parr_2003}). Another
extension has been the introduction of safety features into the
calculations of the new policy (e.g risk
aversion~\cite{howard_metheson_72,marcus_et_al_97,sato_kobayashi_2000}). Such
modifications have been extensively used, particularly in robotics,
leading to more and more advanced policy iteration algorithms (see,
for example, ~\cite{sugiyama_et_al_2009,lagoudakis_parr_2003}). Hence,
by making the basic PI method our study subject in this paper, we
intend to impact a large swathe of applications where these learning
techniques are used in a teacher-learner (or leader-follower) setup.

In more detail, we consider the standard PI algorithm, where the
environment model is completely known and the value function is
directly computed. However, for the reasons of computational
convenience, we do introduce a modification into the policy
improvement stage. Namely, the new policy is computed as a
soft-maximisation with respect to the value function. This is done to
support differentiability of the policy with respect to the
environment dynamics necessary for numerical calculations to solve the
resulting TOP. However, this modification is relatively standard for
PI algorithms (see, for example~\cite{perkins_precup_2003} and
references therein). This is in part due to the fact that PI is
frequently used in combination with a form of neural-network
computation to represent either the policy or the value function, and
in such representations soft-max occurs naturally (see
e.g.~\cite{bertsekas_tsitsiklis_book_96}). The soft-max function we
use is that of Gibbs-Boltzmann, which is extensively used in
neural-network computations, where a vector $\mathbf{v}=(v_1,...,v_k)$
is transformed into a normalised vector $\sigma(\mathbf{v}|\tau)$
proportional to $(\exp(\tau v_1),...,\exp(\tau v_k))$. The parameter
$\tau_t$ denotes a so called {\em temperature} scale that shifts the
soft-max towards the greedy maximum selection, i.e. if we let
\[
\lim\limits_{\tau\rightarrow\infty}\sigma(\mathbf{v}|\tau)=\sigma^*,
\]
then 
\[\sigma^*_j\neq 0 \mbox{ iff } j\in\arg\max\limits_{1\leq i\leq k}v_i.\]


Formally instantiating our learner's state update $F(x_t,u)$ by PI
leads to the following set of equations: %\\\ \\
\begin{description}
\item[Policy evaluation:]
%{\em Policy evaluation:}\\
%\centerline{
\[\hspace{-20pt}V_t(s)=\sum\limits_{s'}T_{u_t}(s'|s,\pi_{t-1}(s))\left[
    c(s',\pi_{t-1}(s),s)+\gamma V_t(s')
    \right]\]
    %}

\item[Policy improvement:]
\[\hspace{-20pt}\pi_t(a|s)=\frac{1}{Z_t(s)}\exp\left(\tau_t\sum\limits_{s'}T_{u_t}(s'|s,a)\left[
    c(s',a,s)+\gamma V_t(s')
    \right]\right) \]
    
\item[Normalisation factor:]
\[\hspace{-20pt}Z_t(s)=\sum\limits_a\exp\left(\tau_t\sum\limits_{s'}T_{u_t}(s'|s,a)\left[
    c(s',a,s)+\gamma V_t(s') \right]\right)\]
\end{description}
    
%{\em Policy improvement:}\\
%\centerline{
%$\pi_t(a|s)=\frac{1}{Z_t(s)}\exp\left(\tau_t\sum\limits_{s'}T_{u_t}(s'|s,a)\left[
%    c(s',a,s)+\gamma V_t(s')
%    \right]\right)$}
%{\em Normalisation factor:}\\ 
%\centerline{
%$Z_t(s)=\sum\limits_a\exp\left(\tau_t\sum\limits_{s'}T_{u_t}(s'|s,a)\left[
%    c(s',a,s)+\gamma V_t(s') \right]\right)$}

Substituting the above into the standard
TOP formulation leads to a TOP-PI optimisation problem depicted in
Figure~\ref{t_opt_PI}.
\begin{figure}[th]
\begin{tabular}{|c|} \hline \parbox{3.2 in} {\center 
$\arg\min\limits_{u_t}\sum\limits_{t=1}^{t_{max}}\sum\limits_{s,a}\pi_t(a|s)q_t(s)D^{KL}_t(s,a)$\\
$s.t.$\\
$V_t(s)=\sum\limits_{s'}T_{u_t}(s'|s,\pi_{t-1}(s))\left[
c(s',\pi_{t-1}(s),s)+\gamma V_t(s')
\right]$\\
$\pi_t(a|s)=\frac{\exp\left(\tau_t\sum\limits_{s'}T_{u_t}(s'|s,a)\left[
c(s',a,s)+\gamma V_t(s')
\right]\right)}{Z_t(s)}$\\
$Z_t(s)=\sum\limits_a\exp\left(\tau_t\sum\limits_{s'}T_{u_t}(s'|s,a)\left[
c(s',a,s)+\gamma V_t(s')
\right]\right)$\\
$\pi_0\ \ \displaystyle{given}$\\
$D^{KL}_t(s,a)=\sum\limits_{s',a'}T_{u_t}(s'|a,s)\pi_t(a'|s')\log\frac{T_{u_t}(s'|a,s)\pi_t(a'|s')}{T^0(s'|a,s)\pi^*(a'|s')}$\\
$q_t(s')=\sum\limits_s\Tilde{T}_{u_t}(s'|s)q_t(s)$\\
$\Tilde{T}_{u_t}(s'|s)=\sum\limits_aT_{u_t}(s'|s,a)\pi_t(a|s)$\\\ \\
}\\ \hline \end{tabular}
\caption{\label{t_opt_PI}TOP-PI: The complete %and explicit 
TOP for the
  PI learner}
\end{figure}
