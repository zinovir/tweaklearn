\section{Conclusions and Future Work}\label{sec: future work}

In this paper we have introduced a novel interaction framework between
a teacher and a learner agents. Unlike previous developments in this
area, in our framework the teacher influences the learner indirectly by
modifying the environment away from some normative, passive
dynamics. We term this process {\em behaviour cultivation}.

Our approach completes the polytomy of feasible teacher-learner
interactions, forming a rigorously studied triad: by demonstration, by
incentive, and by behaviour cultivation. Although the latter two can
be unified under the umbrella of the environment
design~\cite{Zhang09:General}, with some of the ideology tracing back
to Hammond and Converse~\cite{hammond_converse_91}, the members of the
teaching triad are, in fact, distinct. Furthermore, while the
difference in their mathematical formalism can be seen as an outcome
of some representation convenience, the diversity in their
applicability argues that each of them is irrevocably necessary.

%Our approach completes the polytomy of feasible teacher-learner
%interactions, forming a rigorously studied triad: by demonstration, by
%incentive, and by behaviour cultivation. Although the ideology of the
%latter two can be unified under the umbrella of environment
%design~\cite{Zhang09:General}, which in turn can be traced back to at
%least 1991~\cite{hammond_converse_91}, the members of the teaching
%triad are, in fact, distinct. While the difference in their
%mathematical formalism can be seen as an outcome of the representation
%convenience, the diversity in their applicability argues that each of
%them is irrevocably necessary.

In particular, we have provided an experimental domain that could not
be addressed by neither {\em demonstration} (because the teacher's and
the learner's interests in the task contradict) nor currently
available {\em incentive} based methods (because incentives created a
reasoning paradox in the task), but was successfully resolved using
our {\em behaviour cultivation} teaching method. In general, as an
important part of our future work we see the classification of various
teacher-learner tasks into groups that are more suitable for a
particular teaching method.

%As an important part of our future work we see the classification of
%various teacher-learner tasks into groups that are more suitable for a
%particular teaching method. As a part of this effort, we have provided
%an experimental domain that could not be addressed by neither {\em
%  demonstration} (because the teacher's and the learner's interests in
%the task contradict) nor currently available {\em incentive} based
%methods (because incentives created a reasoning paradox in the task),
%but was successfully resolved using our {\em behaviour cultivation}
%teaching method.

Although in this paper we provide an example based on a learner
executing the policy iteration (PI) algorithm, this limitation is not
a part of our Teacher Optimisation Problem (TOP) framework. Rather it
is a particular instantiation of its principles for the PI
algorithm. We hope that the wide applicability of PI-type algorithms
will allow for a faster adoption of our {\em behaviour cultivation}
method by the multi-agent systems research community. As part of our
ongoing research we will investigate the instantiations of TOP with
other learning algorithms, particularly those capable of knowledge
transfer~\cite{taylor_stone_2009,taylor_PhD_2008}.

The cost and the effectiveness of the teaching process in TOP can be
measured simultaneously via the Kullback-Leibler divergence rate
(KLR). Specifically, by measuring the KLR between two state-action
processes engendered by the learner's action policy and the
environment dynamics set by the teacher. The detailed choice of the
two processes depends on the interpretation of this cost. One such
interpretation, as the effort it takes to sustain the teaching process
at any given time, is adopted in this paper. As a result the cost is
the KLR between the current policy-environment combination and the
combination of the reference policy with the passive dynamics.

Although ideologically this choice of the cost function is similar to
works by
Todorov~\cite{todorov_2009_framework,todorov_2009_framework_sup}, its
integration with the interaction model is different. Furthermore, we
use KL {\em rate}, rather than KL {\em divergence} used by
Todorov. This allows us to focus on the {\em long term} divergence
between the two processes, which is consistent with our
Assumption~\ref{assume_persistence} that the learner seeks best
response to the long term effects of the augmented environment
dynamics.

Notably, however, there are several more important and interesting
cost interpretations. For instance, as a part of our ongoing and
future work, we explore an interpretation as the total effort invested
into the teaching process. In this case the KLR will be between two
consecutive policy-environment combinations plus some final cost
expressed by the KLR between the final policy-environment pair and the
combination of the reference policy with the passive environment
dynamics.

