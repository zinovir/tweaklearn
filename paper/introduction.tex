

\section{Introduction}
%\nocite{fleming_hernandez-hernandez_CDC_97}
%\nocite{todorov_2009_framework_sup}
%\nocite{todorov_2009_framework}
%\nocite{ng_russell_2000}
%\nocite{zhang_parkes_2009_ed}
%\nocite{dufton_larson_2009}

There are three general teaching techniques applied by people:
teaching by demonstration, teaching by providing incentives, and
teaching by modifying the underlying environment dynamics.  While the
first two have been successfully mapped into intelligent agent models,
to the best of our knowledge, the third one has yet to be
instantiated.

Teaching by {\em demonstration} has a teacher provide example
state-to-action mappings in order to show the learner what a good
policy would be.  This approach has found great success in
robotics~\cite{argal_etal_2009}. However, most of these works assume
that the learner actually wishes to learn the task, as well as a
certain benevolence on behalf of the teacher with respect to the
learned task.

On the other hand, much of the work involving teaching by using {\em
  incentives} has no need to assume that the teacher's and learner's
initial interests coincide.  In particular, research in this area has
looked at ways in which a teacher could encourage or convince a
learner to follow some desired policy by providing rewards or
punishments.  Recently, Zhang \emph{et al} introduced a general
framework they call \emph{environment
  design}~\cite{Zhang09:General}. In environment design an interested
party attempts to influence the behaviour of an agent by making
limited changes to the agent's environment. Although, in general, this
may include environment dynamics modification, Zhang \emph{et al} have
concentrated on teaching by incentive. In particular, Zhang \emph{et
  al} have allowed their interested party to modify the cost function
of an agent in a linear programming example~\cite{Zhang09:General}, or
to modify the rewards of an agent acting in an environment modelled as
a Markov Decision Problem
(MDP)~\cite{zhang_parkes_2008,Zhang09:Policy}. However, these {\em
  incentive} based approaches in their current form are not
sufficiently flexible. In fact, as one of our experimental domains
demonstrates (see Section~\ref{sec: experiments}), there exist
environments where certain behaviours can not be enforced by the
method of Zhang \emph{et al}.

In this paper we explicitly focus on the implications of allowing the
interested party (teacher, in our model) to modify the \emph{dynamics}
of the environment, while leaving the reward function of the agent
alone. We term this process of teaching {\em behaviour
  cultivation}. In more detail, we concentrate on environments
modelled by the learner as an MDP, and allow the teacher to
\emph{tweak} (\emph{i.e.} make small changes to) the environment
dynamics and record the outcome within the MDP model.  The teacher's
goal is, therefore, to determine the form and the degree of tweaking
necessary to enforce a specified behaviour upon the learner.


While our model may be cast as an example of environment design, we
note that our instantiation differs significantly from the particular
cases studied by Zhang \emph{et al}, and therefore creates a separate
line of study. In fact, representing the teacher's task as a control
problem is far more reminiscent of the work by Banerjee and
Peng~\cite{banerjee_peng_2005}. In~\cite{banerjee_peng_2005} an
additional assumption is made about the size of the learner's memory
and the fact that the teacher-learner interaction is based on a
repeated normal form game. This enabled Banerjee and Peng to enumerate
all possible memory configurations and use them as a system state
space, casting the interaction between the teacher and its opponent as
a planning and control problem termed Adversary MDP. Solving this
problem allows the teacher to force its opponent to follow a strategy
which is most beneficial from the teacher's utility point of
view. However, their method provides no formal way of achieving a
prespecifed behaviour of the learner, as well as being limited to a
finite set of memory configurations, hardly a feasible situation even
for a learning algorithm in a simple repeated normal form game.

We believe that the power of our learning model is best illustrated by
the following real-world scenario. A parent wishes to teach a child to
ride a bicycle. The parent may {\em demonstrate} by riding the
bicycle. However, in practice, this does not yield good results when
the child attempts to repeat the task. It is also possible to promise
an {\em incentive}, be that a candy or a trip to the
park. Unfortunately, although increasing the child's efforts, this
does not facilitate the learning process. The most practical thing to
do, in this case, is to {\em modify the dynamics} -- add safety wheels
to the bicycle. Gradually raising the safety wheels constitutes {\em
  behaviour cultivation}. It ultimately allows the child to accustom
to the complete range of motion possibilities and, eventually, ride an
unabridged bicycle version. Another good example, this time with
multiple agents, would be the task a coach faces when introducing a
new player into his football team. The new player has to be accustomed
to this team's play-book (a set of attack and defence plans), but also
the rest of team has to be trained to incorporate the unique set of
skills brought in by the new player. Here too, neither {\em
  demonstration} nor {\em incentives} work very well. Rather, the
coach has to create a sequence of drills where the new player's skills
and the old play-book will be gradually integrated. These drills do
not possess the complete complexity and dynamics of the real football
game, instead they gradually approximate the real game dynamics, and
constitute {\em behaviour cultivation}. Ultimately, a full scale
football match is played, where the coach no longer influences the
game rules or dynamics.

The contributions of this work are three-fold:
\begin{itemize}
\item We introduce a model whereby a teacher can modify or tweak
  environment dynamics so as to cultivate some desired behaviour in a
  learning agent. This is the first time the {\em behaviour
    cultivation} teaching method is considered explicitly.
\item We introduce a cost function for the teacher that naturally
  incorporates and balances the teacher's effort and the deviation of
  the learner's performance from an ideal reference, that which the
  teacher is interested in. Such balance is an important feature for
  multi-aspect optimisation, and otherwise would have necessitated a
  separate treatment.
\item We instantiate our model with a particular learning agent, and
  then show, empirically, that our model is effective. Our {\em
    behaviour cultivation} method was able both to speed up a normal
  learning process and solve teaching tasks which are hard or
  impossible for other methods.
\end{itemize}

%To achieve this goal, we first introduce a way to measure the
%divergence between the realised and the passive (when no modification
%is applied by the teacher) environment developments in a Markovian
%system. This measure naturally incorporates and balances the teacher's
%effort and the deviation of the learner's performance from an ideal
%reference, that which the teacher is interested in. It also allows us
%to formulate the teacher's problem as a planning and control problem,
%and solve it using classical analytical and numerical tools.

The rest of this paper is organised in the following manner. In
Section~\ref{sec: GeneralModel} we present our general model for
\emph{behaviour cultivation}, and describe the cost function, which is
based on the Kullback-Leibler Rate, that we use.  In Section~\ref{sec:
  TOP-PI} we instantiate our general model with a particular type of
learning agent, one that uses a Policy Iteration algorithm to
determine which policy it will follow. Using this instantiation, we
show, in Section~\ref{sec: experiments}, that our model is effective,
before concluding in Section~\ref{sec: future work} with a discussion
of future research directions.


%In what follows we will formally define {\em behaviour cultivation}
%process that can be parameterised by the learning algorithm we wish to
%teach(Section~\ref{sec: GeneralModel}).
%%teaching by dynamics
%%modification 
%%given a learning algorithm we wish to teach
%%. 
%We will also provide a specialised
%version of the formalism for a specific MDP solution technique --
%Policy Iteration (PI) algorithm (Section~\ref{sec: TOP-PI}). $\{\{$
%Our experiments in Section~\ref{sec: experiments} will compare the
%performance of PI with and without dynamics modification. $\}\}$. We
%will then conclude in Section~\ref{sec: future work} with a discussion
%of further development teaching by dynamics modification.

