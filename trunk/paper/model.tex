

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interaction Model}\label{sec: GeneralModel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we provide a high level description of the problem and
general framework.  In the next section we provide a particular
instantiation of our framework.

Our framework consists of a stochastic environment and two agents, a
\emph{learner} and a \emph{teacher}.  The learner acts within the
environment, taking actions and receiving feedback in the form of
rewards, which depend on the action taken and the current state of the
environment in which the agent finds itself.  We assume that the
learner is rational and thus attempts to find a \emph{policy} which
describes what action to take in each environment state, so as to
maximises its expected reward.  The teacher, on the other hand, does
not act \emph{in} the environment, but rather acts \emph{on} the
environment.  In particular, the teacher has some desired
\emph{reference policy}, $\pi^*$, that it wishes the learner to
follow, but is unable to force the agent to take any particular
action.  Instead, it  it is able to modify the environment's
\emph{dynamics} in order to cultivate the desired behaviour in the learner.  That is, the teacher's actions are able to influence the way
the environment state changes in response to the learner's actions,
and thus influence the policy of the learner. Dynamics modifications, or \emph{tweaks}, 
come at a cost, however, and thus the goal of the teacher is to
minimise the modifications it must make to the environment dynamics
while at the same time ensuring that the policy followed by the
learner is close enough to the desired reference policy.

We represent  the problem with the tuple  $\langle S, A, c,\gamma, U,T\rangle$ where:
\begin{itemize}
\item $S$ is the set of states, 
\item $A$ is the set of actions available to the learner,
\item  $c:S\times A\times S\rightarrow\mathbf{R}$ is the reward (or
  cost) function of the learner. $c(s',a,s)$ is the reward received by
  the learner if it has applied action $a\in A$ and the environment
  moved from state $s\in S$ to state $s'\in S$,
\item $\gamma \in (0,1)$ is a discount factor,   

\item $U$ is the set of actions (modifications to the environment)
  that the teacher can apply where $u_t\in U$ is the modification or tweak made
  at time $t$,

\item $T:S\times A\times U\rightarrow\Delta(S)$ describes the
  environment dynamics where $T_u(s'|s,a)\equiv T(s'|s,a,u)$ is the
  probability that the state will change from $s$ to $s'$ if the
  learner has applied action $a\in A$ and the teacher chose
  environment modification $u\in U$.

We assume that there exists a null modification $u^0\in U$, so that
$T^0=T_{u^0}$  are the original dynamics of the environment before any teacher-modifications. We will use the term \emph{passive dynamics} to refer to $T^0$ to highlight the fact that it is unchanged.

\end{itemize}

We assume that the learner modifies and updates its policy by using some iterative algorithm, such 
as value or policy iteration.  In between iterations, the teacher is able to apply a tweak or 
modification to the environment dynamics, and thus, the learner faces 
 a sequence of Markov Decision Problems (MDPs)~\cite{puterman_book_94},
 given by tuples $<S,A,U,T_{u_t},R>$. We assume that the learner is unaware of the teacher's  
 actions, and thus proceeds as if the sequence of MDPs were homogeneous.
 
 
\begin{assume}\label{assume_persistence}
At every
stage the learner seeks an action policy of the form
$\pi:S\rightarrow\Delta(A)$ that would produce the highest expected
reward if $T_{u_t}$ would persist indefinitely.\footnote{This 
  assumption is explicit only if the learner actually has access to
  the environment model. For most standard reinforcement learning algorithms this
  assumption would hold implicitly.}
\end{assume}


Let $x_t$ represent all information and features that the learner uses
in determining its policy, and let
$\pi_t=\pi(x_t):S\rightarrow\Delta(A)$ be the policy that corresponds
to that state.  Additionally, let $\pi^*$ be the ideal policy that the
teacher desires the learner to follow.  At time $t$, the teacher
incurs a cost, $\mathrm{Cost}(\pi_t,u_t)$, which combines the
difference between the actual policy being followed by the learner,
$\pi_t$, and the desired policy of the teacher, $\pi^*$, with the
amount of environmental modifications the learner has had to make in
order to maintain the current dynamics, $T_{u_t}$, compared to the
initial environment dynamics, $T^0$. If we let $x_t=F(x_{t-1},u_t)$
denote one step of the learner's policy-determination algorithm and
assume that $x_0$, the original state of the learner,  is known, then it is possible to formulate the overall optimisation problem faced by the teacher.
In particular, it is:
\begin{eqnarray*}
&\min\limits_{u_t}\sum\limits_{t=1}^{t_{max}}Cost(\pi_t,u_t)\\
&s.t.\\
&\pi_t=\pi(x_t)\\
&x_t=F(x_{t-1},u_t).
\end{eqnarray*}



%Now, denote $x_t$ the internal state of the learner's policy
%computation at iteration $t$, and let
%$\pi_t=\pi(x_t):S\rightarrow\Delta(A)$ be the policy that corresponds
%to that computation state. Also, denote $\pi^*$ the ideal reference
%action policy of the learner. Then at time $t$ the teacher incurs cost
%$Cost(\pi_t,u_t)$ that combines the distance between $\pi_t$ and
%$\pi^*$ and between $T_{u_t}$ and $T^0$. The overall optimisation
%problem for the teacher is as follows, where $x_t=F(x_{t-1},u_t)$
%denotes one step of the learner's computation algorithm and $x_0$ is
%known:
%\begin{eqnarray*}
%&\min\limits_{u_t}\sum\limits_{t=1}^{t_{max}}Cost(\pi_t,u_t)\\
%&s.t.\\
%&\pi_t=\pi(x_t)\\
%&x_t=F(x_{t-1},u_t)
%\end{eqnarray*}

This formulation of the teacher's optimization function is fully generic. It does not explicitly specify 
the learner's algorithm beyond assuming that it is iterative. This formalism thus captures both policy 
and value iteration algorithms, both with given and learned environment models.
It can also
 capture the case where the learner
is capable of transfer learning~\cite{ taylor_stone_2009}. In this scenario the learner's state $x_t$
will include structural knowledge gathered thus far from the
interaction with the environment. 

Our teacher-learner interaction framework, as described, is also generic with respect to the 
instantiation of the teacher's cost function, $Cost(\pi_t,u_t)$. We argue that suitable cost functions 
for this problem should incorporate information as to what environmental modifications the teacher 
has performed (\emph{i.e.} the actions taken by the teacher), along with information about how 
similar the policy of the learner is to the desired policy of the teacher (\emph{i.e.} how far the 
teacher is from its goal). While any function which combines these features in a meaningful way 
would work, in this paper we adopt a specific cost function derived from the \emph{Kullback-Leibler 
Divergence Rate}.    We describe our proposed cost function in more detail in the next section.



\subsection{Teacher's Cost Computation}\label{sec:KLCost}

In this section we describe our cost function, based on the 
 Kullback-Leibler Divergence Rate (KL Rate).  We first provide some background material on 
 Kullback-Leibler Divergence (KL Divergence) and KLR. We then describe how our cost function is 
 defined, 
 and provide an argument as to why it is appropriate for our setting.


\begin{Definition}
Let $p$ and $q$ be probability distributions over some discrete random variable. Then the Kullback-Leibler (KL) Divergence of $q$ from $p$  is
\[
D^{KL}(p||q)=\sum_i p(i)\log \frac{p(i)}{q(i)}.
\]
\end{Definition}

\noindent Informally, KL Divergence measures the difference between two probability distributions.
KL Rate is an extension of KL Divergence to Markov processes.
\begin{Definition}
Let $\{X^1_t\}$ and $\{X^2_t\}$ be Markov Processes.  The Kullback-Leibler (KL) Rate is
\[
KLR(X^1||X^2)=\lim_{n\rightarrow \infty} \frac{1}{n}DL^{KL}(P(X^1=x_n)||P(X^2=x_n)).
\]
\end{Definition}
If the processes can be described by two conditional transition matrices, $P$ and $Q$, where $P(x'|x)$ (and  respectively $Q(x'|x)$) is the probability of transitioning from state $x$ to state $x'$, then
\[
KLR(X^1||X^2)=\sum_{x} D^{KL}(P(\cdot|x)||Q(\cdot|x))p_{stat}(x)
\]
where $p_{stat}$ is the stationary distribution of $P$~\cite{rached_alajaji_campbell_2004}.


Now, returning to our problem, let $\pi_t$ be the policy of the learner at time $t$ and let $T_{u_t}$ 
be the dynamics of the environment after the teacher has applied tweak $u_t$. If $\pi_t$ was to be 
repeatedly used in the environment modified by $u_t$, this would result in a homogeneous 
Markovian process defined by the transition matrix
\[
P_t(s',a'|s,a)=T_{u_t}(s'|s,a)\pi_t(a'|s').
\]
Similarly, we can define another  Markovian process by the transition matrix
\[
P^*(s',a'|s,a)=T^0(s'|s,a)\pi^*(a'|s').
\]
This is the ideal stochastic process over state-action pairs, from the teacher's perspective. In 
particular, it is formed when the teacher's desired policy, $\pi^*$, is followed by the learner in the 
original, unmodified environment. That is, the learner executes the teacher's desired policy with no 
intervention from the teacher.

Assuming that $P_t$ and $P^*$ are irreducible with respect to $S\times A$,  we define our cost function as
\begin{eqnarray*}
Cost(u_t,\pi_t) & =& KLR(P_t||P^*) \\
                           &=& \sum_{s,a}D^{KL}_t(s,a)q_t(s,a)
\end{eqnarray*}
where
\[
D_t^{KL}(s,a)=D^{KL}(P_t(\cdot,\cdot|s,a)||P^*(\cdot,\cdot|s,a))
\]
and $q_t(s,a)$ is the  stationary distribution of $P_t$, so that
$q_t=P_tq_t$. 
Notably, the
stationary distribution can be decomposed (with a slight abuse of
notation) to be $q_t(s,a)=q_t(a|s)q_t(s)$ and then expressed by the
following equations:
%% \begin{eqnarray*}
%% q_t&=&q_t(a'|s')q_t(s')=P_tq_t\\
%% &=&\sum\limits_{s,a}T_{u_t}(s'|s,a)\pi_t(a'|s')q_t(a|s)q_t(s)\\
%% &=&\pi_t(a'|s')\sum\limits_sq_t(s)\sum\limits_aT_{u_t}(s'|s,a)q_t(a|s)\\
%% &&\{\displaystyle{substitute}\ \ q_t(\cdot|\cdot)\Leftarrow\pi_t(\cdot|\cdot)\}\\
%% &=&\pi_t(a'|s')\sum\limits_sq_t(s)\sum\limits_aT_{u_t}(s'|s,a)\pi_t(a|s)\\
%% &&\{\displaystyle{denote}\ \ \Tilde{T}_{u_t}(s'|s)=\sum\limits_aT_{u_t}(s'|s,a)\pi_t(a|s)\}\\
%% &=&\pi_t(a'|s')\sum\limits_sq_t(s)\Tilde{T}_{u_t}(s'|s)
%% \end{eqnarray*}
%% so that
\begin{eqnarray*}
q_t(s',a')&=&\pi_t(a'|s')q_t(s')\ \ \mbox{where}\\
q_t(s')&=&\sum\limits_s\Tilde{T}_{u_t}(s'|s)q_t(s) \ \ \mbox{and}\\
\Tilde{T}_{u_t}(s'|s)&=&\sum_{a}T_{u_t}(s'|s,a)\pi_t(a|s).
\end{eqnarray*}



Incorporating our KLR-based cost function, the overall generic Teacher
Optimisation Problem (TOP) is depicted in Figure~\ref{t_opt}. 

Notice
that this formulation retains complete flexibility with respect to the
specific algorithm selected by the learner to optimise its
policy. Nevertheless, to provide further intuition and demonstrate the
feasibility of the approach, in the rest of this paper we instantiate
the algorithm $F$ to be the Policy Iteration algorithm. However,
before proceeding  to this particular instantiation, we would like to
remark upon the meaning of the use of KLR and KL Divergence in our teaching problem.

%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
\begin{tabular}{|c|} \hline \parbox{3.2 in} {\center 
$\arg\min\limits_{u_t}\sum\limits_{t=1}^{t_{max}}\sum\limits_{s,a}\pi_t(a|s)q_t(s)D^{KL}_t(s,a)$\\
s.t.\\
$\pi_t=\pi(x_t)$\\
$x_t=F(x_{t-1},u_t)$\\
$x_0\ \ \displaystyle{is\ \ given}$\\
$D^{KL}_t(s,a)=\sum\limits_{s',a'}T_{u_t}(s'|a,s)\pi_t(a'|s')\log\frac{T_{u_t}(s'|a,s)\pi_t(a'|s')}{T^0(s'|a,s)\pi^*(a'|s')}$\\
$q_t(s')=\sum\limits_s\Tilde{T}_{u_t}(s'|s)q_t(s)$\\
$\Tilde{T}_{u_t}(s'|s)=\sum\limits_aT_{u_t}(s'|s,a)\pi_t(a|s)$\\\ \\
}\\ \hline \end{tabular}
\caption{\label{t_opt}The complete generic TOP}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As we mentioned in the start of this section, KL Divergence,
$D^{KL}(P||Q)$, informally measures the difference between some
factual distribution $P$ and some desired distribution
$Q$.\footnote{Note that while KL Divergence is often called the
  \emph{distance} between two distributions, it is, in fact, not a
  true distance metric.}  As expected, $D^{KL}(P||Q)$ is minimized
exactly when $P=Q$, that is, $D^{KL}(P||P)=0$. Importantly, it
compares two distributions, rather than distribution properties, such
as the mean or variance which are commonly used. \footnote{Consider,
  for instance, the optimality criterea of MDPs: the {\em expected}
  accumulated reward. Rather than being concerned with the entire
  shape of the obtainable rewards, it only concentrates on the
  expecations, neccessitating further solution augmentation to account
  for requirements such as risk-aversion.}. In our problem, the
desired distribution is $P^*(s',a'|s,a)$ which arises if the learner
follows the teacher's desired policy with no environmental tweaks
required.  By using KLR as the cost function, we are able to assign
costs to the complete variety of possible long term deviations caused
by departing from the ideal probability of state-action pair
transition.  These deviations may arise from either the environmental
tweak made by the teacher, or by the learner following a non-desired
policy, or some combination of both, and thus our cost function
balances these appropriately.

%Originally, $KL$ divergence (and KRL) was designed to measure the cost
%of mistaken encoding. That is, if a signal had come from a
%distribution $P$, while we have used a distribution $Q$ to encode it,
%$KL(P\|Q)$ measures the extra bits we had to
%use\cite{cover_thomas_IT_book_91}. Therefore, given the signal,
%information theory seeks to recover $Q$ to minimise the extra cost. In
%our teaching domain, the signal is the sequence of state-action pairs
%engendered by the system dynamics and the learning algorithm. However,
%by changing the system dynamics we essentially change the signal true
%distribution, rather than its encoding. Hence, we represent the ideal
%policy and the passive dynamics as the encoding distribution $Q$,
%while we modulate the signal distribution $P$ to match it.




