

\section{Introduction}
%\nocite{fleming_hernandez-hernandez_CDC_97}
%\nocite{todorov_2009_framework_sup}
%\nocite{todorov_2009_framework}
%\nocite{ng_russell_2000}
%\nocite{zhang_parkes_2009_ed}
%\nocite{dufton_larson_2009}

There are three general teaching techniques applied by people:  teaching by
demonstration, teaching  by  providing incentives,  and teaching by modifying the underlying environment dynamics.  While the
first two have been successfully mapped into intelligent agent models, to the best
of our knowledge, the third one has yet to be instantiated.

Teaching by {\em demonstration}  has a teacher provide  example state-to-action mappings in order  to show the learner what a good policy would be.  This approach  has  found great success in
robotics~\cite{argal_etal_2009}. However, most of these works assume
that the learner actually wishes to learn the task, as well as a
certain benevolence on behalf of the teacher with respect to the
learned task.

On the other hand, much of the work involving teaching by  using {\em incentives} 
has not need to assume that the teacher's and learner's initial interests coincide. 
In particular, research in this area has looked at ways in which a teacher could encourage or convince a learner to follow some desired policy by providing rewards or punishments.
 Recently, Zhang \emph{et al}
introduced a general framework they call \emph{environment
  design}~\cite{Zhang09:General}. In environment design an interested
party attempts to influence the behaviour of an agent by making limited
changes to the agent's environment. Although, in general, this may
include environment dynamics modification, Zhang \emph{et al} have
concentrated on teaching by incentive. In particular, Zhang \emph{et
  al} have allowed their interested party to modify the cost function
of an agent in a linear programming example~\cite{Zhang09:General}, or
to modify the rewards of an agent acting in an environment modelled as
a Markov Decision Problem (MDP)~\cite{zhang_parkes_2008,Zhang09:Policy}.

In this paper we explicitly focus on the  implications of allowing
the interested party (teacher, in our model) to modify the
\emph{dynamics} of the environment, while leaving the reward function
of the agent alone. We term this process of teaching {\em behaviour
  cultivation}. In more detail, we concentrate on environments
modelled by the learner as an MDP, and allow the teacher to \emph{tweak} (\emph{i.e.} make small changes to)  the
environment dynamics and record the outcome within the MDP model. 
The teacher's goal is, therefore, to determine the form and the degree of
tweaking necessary to enforce a specified behaviour upon the
learner. 


While our model may be cast as an example of environment design, we
note that our instantiation differs significantly from the particular
cases studied by Zhang \emph{et al}, and therefore creates a separate
line of study. In fact, representing the teacher's task as a control
problem is far more reminiscent of the work by Banerjee and
Peng~\cite{banerjee_peng_2005} [ZINOVI: HAD YOU PUT A QUICK LINE AS TO WHAT THIS PAPER IS ABOUT? I DO NOT KNOW IT], although it too focuses on the reward
modification.

We believe that the power of our learning model is best illustrated by the following real-world scenario. A
parent wishes to teach a child to ride a bicycle. The parent may {\em
  demonstrate} by riding the bicycle. However, in practice, this does
not yield good results when the child attempts to repeat the task. It
is also possible to promise an {\em incentive}, be that a candy or a
trip to the park. Unfortunately, although increasing the child's
efforts, this does not facilitate the learning process. The most
practical thing to do, in this case, is to {\em modify the dynamics} --
add safety wheels to the bicycle. Gradually raising the safety wheels
constitues {\em behaviour cultivation}. It ultimately allows the child
to accustom to the complete range of motion possibilities and,
eventually, ride an unabridged bicycle version.

The contributions of this work are three-fold:
\begin{itemize}
\item We introduce a model whereby a teacher can modify or tweak environment dynamics so as to cultivate some desired behaviour in a learning agent.
\item We introduce a  cost function for the teacher that
naturally incorporates and balances the teacher's
effort and the deviation of the learner's performance from an ideal
reference, that which the teacher is interested in.
\item We instantiate our model with a particular learning agent, and then show, empirically, that our model  is effective.
\end{itemize}

%To achieve this goal, we first introduce a way to measure the
%divergence between the realised and the passive (when no modification
%is applied by the teacher) environment developments in a Markovian
%system. This measure naturally incorporates and balances the teacher's
%effort and the deviation of the learner's performance from an ideal
%reference, that which the teacher is interested in. It also allows us
%to formulate the teacher's problem as a planning and control problem,
%and solve it using classical analytical and numerical tools.

The rest of this paper is organized in the following manner. In Section~\ref{sec: GeneralModel} we
present our general model for \emph{behavior cultivation}, and describe the cost function, which is based on the Kullback-Leibler Rate, that we use.  In Section~\ref{sec: TOP-PI} we instantiate our general model with a particular type of learning agent, one that uses a Policy Iteration algorithm to determine which policy it will follow. Using this instantiation, we show, in Section~\ref{sec: experiments},  that our model is effective, before concluding in Section~\ref{sec: future work} with a discussion of future research directions.


%In what follows we will formally define {\em behaviour cultivation}
%process that can be parameterised by the learning algorithm we wish to
%teach(Section~\ref{sec: GeneralModel}).
%%teaching by dynamics
%%modification 
%%given a learning algorithm we wish to teach
%%. 
%We will also provide a specialised
%version of the formalism for a specific MDP solution technique --
%Policy Iteration (PI) algorithm (Section~\ref{sec: TOP-PI}). $\{\{$
%Our experiments in Section~\ref{sec: experiments} will compare the
%performance of PI with and without dynamics modification. $\}\}$. We
%will then conclude in Section~\ref{sec: future work} with a discussion
%of further development teaching by dynamics modification.

